cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
##################################
#####Simple Random Imputation#####
##################################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
set.seed(0)
imputed.x2 = impute(missing.data$x2, "random") #Simple random imputation using the
#impute() function from the Hmisc package.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
#############################
#####K-Nearest Neighbors#####
#############################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
imputed.1nn = kNN(missing.data, k = 1) #Imputing using 1NN.
imputed.5nn = kNN(missing.data, k = 5) #Imputing using 5NN.
imputed.9nn = kNN(missing.data, k = 9) #Imputing using 9NN.
imputed.1nn #Inspecting the imputed values of each of the methods;
imputed.5nn #what is going on here? Given our dataset, should we
imputed.9nn #expect these results?
#K-Nearest Neighbors regression on the sleep dataset.
sqrt(nrow(sleep)) #Determining K for the sleep dataset.
sleep.imputed8NN = kNN(sleep, k = 8) #Using 8 nearest neighbors.
sleep.imputed8NN
sleep.imputed8NN
summary(sleep) #Summary information for the original sleep dataset.
summary(sleep.imputed8NN[, 1:10]) #Summary information for the imputed sleep dataset.
#K-Nearest Neighbors classification on the iris dataset.
help(iris) #Inspecting the iris measurement dataset.
iris
iris.example = iris[, c(1, 2, 5)] #For illustration purposes, pulling only the
#sepal measurements and the flower species.
#Throwing some small amount of noise on top of the data for illustration
#purposes; some observations are on top of each other.
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
missing.vector = c(41:50, 91:100, 141:150) #Inducing missing values on the Species
iris.example$Species[missing.vector] = NA  #vector for each category.
iris.example
col.vec[missing.vector] = "purple" #Creating a new color vector to
#mark the missing values.
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
plot.tile.list(tile.list(info),
fillcol = col.vec[-missing.vector],
main = "Iris Voronoi Tessellation\nDecision Boundaries")
#Adding the observations that are missing species information.
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = 16, col = "white")
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = "?", cex = .66)
#Conducting a 1NN classification imputation.
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
#Conducting a 12NN classification imputation based on the square root of n.
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed12NN$Species)
##################################################
#####Using Minkowski Distance Measures in KNN#####
##################################################
library(kknn) #Load the weighted knn library.
#Separating the complete and missing observations for use in the kknn() function.
complete = iris.example[-missing.vector, ]
missing = iris.example[missing.vector, -3]
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
iris.manhattan = kknn(Species ~ ., complete, missing, k = 12, distance = 1)
summary(iris.manhattan)
help(cars)
cars #Investigating the cars dataset.
#Basic numerical EDA for cars dataset.
summary(cars) #Five number summaries.
sapply(cars, sd) #Standard deviations.
cor(cars) #Correlations.
#Basic graphical EDA for cars dataset.
hist(cars$speed, xlab = "Speed in MPH", main = "Histogram of Speed")
hist(cars$dist, xlab = "Distance in Feet", main = "Histogram of Distance")
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
#Manual calculation of simple linear regression coefficients.
beta1 = sum((cars$speed - mean(cars$speed)) * (cars$dist - mean(cars$dist))) /
sum((cars$speed - mean(cars$speed))^2)
beta0 = mean(cars$dist) - beta1*mean(cars$speed)
#Adding the least squares regression line to the plot.
abline(beta0, beta1, lty = 2)
#Calculating the residual values.
residuals = cars$dist - (beta0 + beta1*cars$speed)
#Note the sum of the residuals is 0.
sum(residuals)
#Visualizing the residuals.
segments(cars$speed, cars$dist,
cars$speed, (beta0 + beta1*cars$speed),
col = "red")
text(cars$speed - .5, cars$dist, round(residuals, 2), cex = 0.5)
#################################################
#####Automatic example with the cars dataset#####
#################################################
model = lm(dist ~ speed, data = cars) #Use the linear model function lm() to
#conduct the simple linear regression.
summary(model) #All the summary information for the model in question. Reports:
#-The five number summary of the residuals.
#-The coefficient estimates.
#-The coeffiient standard errors.
#-The t-test for significance of the coefficient estimates.
#-The p-values for the significance tests.
#-The level of significance.
#-The RSE and degrees of freedom for the model.
#-The coefficient of determination, R^2.
#-The overall model F-statistic and corresponding p-value.
#The equation of the model can be constructed from the output:
#Predicted Distance = -17.6 + (3.9)*Speed
#The interpretation for the slope coefficient: With a 1 MPH increase in car speed,
#the stopping distance, on average, increases by approximately 3.9 feet.
#The interpretation for the intercept coefficient: When a car's speed is 0 MPH,
#the stopping distance, on average, is -17.6 MPH. Theoretically, does this make
#sense? Why might this be the case?
#The residual standard error is about 15.38; this is an approximation of how much
#the residuas tend to deviate around the regression line.
#The coefficient of determination is about 0.65; approximately 65% of the variability
#in the distance variable is explained by the speed variable.
#The intercept, slope, and overall regression is extremely significant (p-values
#all below 0.05).
#Notice that the F-statistic value for the overall regression is the same as the
#square of the t-statistic value for the speed coefficient:
t.statistic = 9.464
f.statistic = 89.57
t.statistic^2
confint(model) #Creating 95% confidence intervals for the model coefficients.
summary(model) #All the summary information for the model in question. Reports:
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
abline(model, lty = 2)
#Constant Variance & Independent Errors
plot(model$fitted, model$residuals,
xlab = "Fitted Values", ylab = "Residual Values",
main = "Residual Plot for Cars Dataset")
abline(h = 0, lty = 2)
#Normality
qqnorm(model$residuals)
qqline(model$residuals)
#Using the built-in plot() function to visualize the residual plots.
plot(model) #Note the addition of the loess smoother and scale-location plot
library(car) #Companion to applied regression.
influencePlot(model)
#####################################
#####Predicting New Observations#####
#####################################
model$fitted.values #Returns the fitted values.
newdata = data.frame(speed = c(15, 20, 25)) #Creating a new data frame to pass
#to the predict() function.
predict(model, newdata, interval = "confidence") #Construct confidence intervals
#for the average value of an
#outcome at a specific point.
predict(model, newdata, interval = "prediction") #Construct prediction invervals
#for a single observation's
setwd("~/Dropbox/NYCDSA/Homework/20151015 - Time Series Analysis")
install.packages("TSA")
library(TSA)
library(forecast)
library(tseries)
data(hare)
ndiffs(hare)
adf.test(hare)
plot(hare)
Acf(hare)
Pacf(hare)
Acf(hare)
Pacf(hare)
fit200 = Arima(hare, c(2, 0, 0))
summary(fit200)
sig = function(model){
return((1 - pnorm(abs(model$coef)/sqrt(diag(model$var.coef)))) * 2)
}
sig(fit200)
fit200 = Arima(hare, c(2, 0, 0))
summary(fit200)
sig(fit200)
fit300 = Arima(hare, order = c(3, 0, 0))
View(hare)
summary(fit300)
sig(fit300)
Acf(hare)
Pacf(hare)
Acf(hare)
sig = function(model){
return(round((1 - pnorm(abs(model$coef)/sqrt(diag(model$var.coef)))) * 2, 3))
}
sig(fit300)
fit201 = Arima(hare, order = c(2, 0, 1))
summary(fit201)
sig(fit201)
fit301 = Arima(hare, order = c(3, 0, 1))
summary(fit301)
sig(fit301)
fit202 = Arima(hare, order = c(2, 0, 2))
summary(fit202)
sig(fit202)
View(sig)
View(sig)
AIC(fit200, fit300, fit201)
BIC(fit200, fit300, fit201)
str(fit200)
summary(fit200)
summary(fit300) #
summary(fit201) #RMSE
fit200$coef
plot(as.vector(fitted(fit200)), fit200$residuals)
abline(h = 0, lty = 2)
qqnorm(fit200$residuals)
qqline(fit200$residuals)
Acf(fit200$residuals)
summary(fit200)
Box.test(fit200$residuals)
Box.test(fit200$residuals, type = "Ljung-Box") #
new.values <- forecast(fit200, 10, level = c(60, 80, 95))
new.values
plot(new.values)
fit200$coef
fitARIMAModel = function(df){
plot(df, main = "The Neimeth Package Analyzing Dataset")
adf.test(df)
#find optimal DVAL by checking all the ADF test until P values is less than 0.05 (SignificantPValue)
for (dval in 0:maxDiff){
if(dval==0){
ddf=df
}else{
ddf <- diff(df,differences=dval)
}
adf.test(ddf)
Acf(ddf)
Pacf(ddf)
print(paste("----Testing Dickey-Fuller test with D=",dval))
print(adf.test(ddf))
if (adf.test(ddf)[4]$p.value < SignificantPValue){
print(paste("P Value is below significance level ",SignificantPValue,". Selecting a Dval = ",dval,sep=""))
break
}
}
#Also called the wald test
sig = function(model) {
return(round((1 - pnorm(abs(model$coef)/sqrt(diag(model$var.coef))))*2, 3))
}
someData <- rep(0, maxpval*maxdval*maxqval*4)
ArimaPDQAIC <- array(someData, c(maxpval, maxqval,6)); #1=name, #2=aic, =3=bic
#ArimaPDQVAL <- array(someData, c(maxpval+1, maxdval+1, maxqval+1,1));
for (pval in 1:maxpval){
for (qval in 1:maxqval){
#The Wald Test
waldTest <- sig(Arima(df, order = c((pval-1), dval, (qval-1))))
waldTest[is.nan(waldTest)] <- 0
#Need to catch for a weird bug when the sig function returns a vector of length 0 because R
if(length(waldTest)==0){
waldTest <- 0
maxWaldTestNoIntercept <- 0
} else{
maxWaldTestNoIntercept <- max(waldTest[1:(length(waldTest)-1)])
}
if(maxWaldTestNoIntercept > SignificantWaldPValue){
print("MODEL NOT SIGNIFICANT")
}
#      ArimaPDQVAL[pval,dval,qval] <-
ArimaPDQAIC[pval,qval,1] <- paste("Model= P=",(pval-1),", D=",dval,", Q=",(qval-1),sep="")
ArimaPDQAIC[pval,qval,2] <- Arima(df, order = c((pval-1), dval, (qval-1)))[6]$aic   #AIC Value
ArimaPDQAIC[pval,qval,3] <- Arima(df, order = c((pval-1), dval, (qval-1)))[15]$aicc #AICC Value
ArimaPDQAIC[pval,qval,4] <- Arima(df, order = c((pval-1), dval, (qval-1)))[16]$bic  #BIC Value
ArimaPDQAIC[pval,qval,5] <- summary(Arima(df, order = c((pval-1), dval, (qval-1))))[2]   #RMSE Value
ArimaPDQAIC[pval,qval,6] <- maxWaldTestNoIntercept  #wald test
#        ArimaPDQAIC[pval,(dval+1),qval,5] <- paste(sig(Arima(ddf, order = c((pval-1), dval, (qval-1)))))
}
}
#deletes models where Wald Test is insignificant
ArimaPDQAIC[ArimaPDQAIC[,,6]>=SignificantWaldPValue] <- 99999999999  #make all values insignifiant when wald test failes
print("columns are Best Model,       AIC,                AICC,                  BIC,                   RMSE,          Max WALD Score")
print(ArimaPDQAIC[ArimaPDQAIC[,,2]==min(ArimaPDQAIC[,,2])])  #Min AIC
print(ArimaPDQAIC[ArimaPDQAIC[,,3]==min(ArimaPDQAIC[,,3])])  #Min AICC
print(ArimaPDQAIC[ArimaPDQAIC[,,4]==min(ArimaPDQAIC[,,4])])  #Min BIC
print(ArimaPDQAIC[ArimaPDQAIC[,,5]==min(ArimaPDQAIC[,,5])])  #Min RMSE
#FIND THE BEST MODEL WHERE THE AIC IS MIN
#bestAIC <- ArimaPDQAIC[ArimaPDQAIC[,,2]==min(ArimaPDQAIC[,,2])][1]
#bestAICC <- ArimaPDQAIC[ArimaPDQAIC[,,3]==min(ArimaPDQAIC[,,3])][1]
#bestBIC <- ArimaPDQAIC[ArimaPDQAIC[,,4]==min(ArimaPDQAIC[,,4])][1]
#bestRMSE <- ArimaPDQAIC[ArimaPDQAIC[,,5]==min(ArimaPDQAIC[,,5])][1]
#print (paste("dval=",dval,", PDQ=",bestAIC,sep=""))
return(ArimaPDQAIC)
}
library(TSA)
library(forecast)
library(tseries)
SignificantPValue <<- 0.075
SignificantWaldPValue <<- 0.05
#max values to test
maxDiff <<- 10
maxpval <<- 6
maxqval <<- 6
df <- hare
ArimaPDQAIC <- fitARIMAModel(df)
maxdval <<- 10
df <- hare
ArimaPDQAIC <- fitARIMAModel(df)
ArimaPDQAIC
df <- WWWusage
ArimaPDQAIC <- fitARIMAModel(df)
class(portfolio)
data("WWWusage")
WWWusage
plot(WWWusage)
ndiffs(WWWusage)
WWWusageDif <- diff(WWWusage, differences = 1)
ndiffs(WWWusageDif)
ndiffs(WWWusageDif)
adf.test(WWWusageDif)
WWWusageDif2 <- diff(WWWusageDif, differences = 1)
adf.test(WWWusageDif2)
Acf(WWWusageDif2)
Pacf(WWWusageDif2)
Acf(WWWusageDif2)
Pacf(WWWusageDif2)
Pacf(WWWusageDif2)
Acf(WWWusageDif2)
fit121 = Arima(WWWusageDif2, c(1, 2, 1))
summary(fit121)
sig(fit121)
fit221 = Arima(WWWusageDif2, order = c(2, 2, 1))
summary(WWWusageDif2)
sig(WWWusageDif2) #p = 3, coefficients are not significant, will use p = 2 and move onto q
summary(WWWusageDif2)
fit221 = Arima(WWWusageDif2, order = c(2, 2, 1))
summary(WWWusageDif2)
sig(WWWusageDif2) #p = 3, coefficients are not significant, will use p = 2 and move onto q
summary(fit221)
sig(fit221) #p = 3, coefficients are not significant, will use p = 2 and move onto q
fit202 = Arima(hare, order = c(2, 0, 2))
summary(fit202)
sig(fit202) #2nd ma term is not significant, revert back to fit201
sig(fit221) #p = 3, coefficients are not significant, will use p = 2 and move onto q
fit222 = Arima(WWWusageDif2, order = c(2, 2, 2))
fit222 = Arima(WWWusageDif2, order = c(2, 2, 2))
summary(fit222)
sig(fit222) #ma term is not significant, however it is low enough we will leave it an add another ar term
AIC(fit121, fit221, fit222)
BIC(fit121, fit221, fit222)
summary(fit121) #RMSE 13.29
summary(fit221) #RMSE 12.83
summary(fit222) #RMSE 12.78
fit222$coef
plot(WWWusage)
fit222$coef
plot(as.vector(fitted(fit222)), fit222$residuals)
abline(h = 0, lty = 2)
qqnorm(fit222$residuals)
qqline(fit222$residuals)
Acf(fit222$residuals) #nothing significant here, independence is okay
Box.test(fit222$residuals, type = "Ljung-Box") #p-value is insignificant therefore we can retain the null hypothesis that the correlations among the residuals are zero, therefore the model is sufficient.
new.values <- forecast(fit222, 10, level = c(60, 80, 95))
new.values
plot(new.values)
WWWusage
nhtemp
nhtemp <- nhtemp
plot(nhtemp)
ndiffs(nhtemp)
nhtempDif <- diff(nhtemp, differences = 1)
ndiffs(nhtempDif)
adf.test(nhtempDif)
Pacf(nhtempDif)
Acf(nhtempDif)
fit110 = Arima(nhtempDif, c(1, 1, 0))
summary(fit110)
sig(fit110)
sig(fit110)
fit210 = Arima(nhtempDif, c(2, 1, 0))
summary(fit210)
sig(fit210) #p = 3, coefficients are not significant, will use p = 2 and move onto q
fit210 = Arima(nhtempDif, c(1, 1, 1))
summary(fit210)
sig(fit210) #p = 3, coefficients are not significant, will use p = 2 and move onto q
AIC(fit110)
BIC(fit110)
summary(fit110) #RMSE 13.29
fit200$coef
fit110$coef
plot(as.vector(fitted(fit110)), fit110$residuals)
abline(h = 0, lty = 2)
qqnorm(fit110$residuals)
qqline(fit110$residuals)
Acf(fit110$residuals) #nothing significant here, independence is okay
Box.test(fit110$residuals, type = "Ljung-Box") #p-value is insignificant therefore we can retain the null hypothesis that the correlations among the residuals are zero, therefore the model is sufficient.
Acf(fit110$residuals) #nothing significant here, independence is okay
fit111 = Arima(nhtempDif, c(1, 1, 1))
summary(fit111)
sig(fit111)
fit211 = Arima(nhtempDif, c(2, 1, 1))
summary(fit211)
sig(fit211)
fit212 = Arima(nhtempDif, c(2, 1, 2))
summary(fit212)
sig(fit212)
fit112 = Arima(nhtempDif, c(1, 1, 2))
summary(fit112)
sig(fit112)
fit113 = Arima(nhtempDif. c(1, 1, 3))
fit113 = Arima(nhtempDif, c(1, 1, 3))
summary(fit113)
sig(fit113)
fit114 = Arima(nhtempDif, c(1, 1, 4))
summary(fit114)
sig(fit114)
AIC(fit110, fit111, fit112, fit113)
BIC(fit110, fit111, fit112, fit113)
summary(fit110) #RMSE 1.77
summary(fit111)
summary(fit112)
summary(fit113)
fit113$coef
plot(as.vector(fitted(fit113)), fit113$residuals)
abline(h = 0, lty = 2)
qqnorm(fit113$residuals)
qqline(fit113$residuals)
Acf(fit113$residuals) #nothing significant here, independence is okay
Box.test(fit113$residuals, type = "Ljung-Box") #p-value is insignificant therefore we can retain the null hypothesis that the correlations among the residuals are zero, therefore the model is sufficient.
new.values <- forecast(fit113, 10, level = c(60, 80, 95))
new.values
plot(new.values)
setwd("~/Dropbox/NYCDSA/Homework/20151015 - Time Series Analysis")
brobs = read.csv('07brobs.csv')
brobs = ts(brobs[119,2], start = 1966, frequency = 12)
brobs
plot(brobs)
brobs = ts(brobs[-119, 2], start = 1966, frequency = 12)
brobs = read.csv('07brobs.csv')
brobs = ts(brobs[-119, 2], start = 1966, frequency = 12)
brobs
plot(brobs)
brobslog <- log(brobs)
plot(brobslog)
ndiffs(brobslog)
brobslogDif <- diff(brobslog, differences = 1)
ndiffs(brobslogDif)
adf.test(brobslogDif)
Pacf(brobslogDif)
Acf(brobslogDif)
fit200 = Arima(brobslogDif, c(2, 0, 0))
summary(fit200)
sig(fit200)
fit300 = Arima(brobslogDif, order = c(3, 0, 0))
summary(fit300)
sig(fit300) #p = 3, coefficients are not significant, will use p = 2 and move onto q
fit201 = Arima(brobslogDif, order = c(2, 0, 1))
summary(fit201)
sig(fit201) #ma term is not significant, however it is low enough we will leave it an add another ar term
fit101 = Arima(brobslogDif, order = c(1, 0, 1))
summary(fit101)
sig(fit101) #third ar term is not significant, so we dont want to add the third ar term, try another ma term
fit102 = Arima(brobslogDif, order = c(1, 0, 2))
summary(fit102)
sig(fit102) #2nd ma term is not significant, revert back to fit201
sig(fit200)
sig(fit300) #p = 3, coefficients are not significant, will use p = 2 and move onto q
sig(fit201) #ma term is not significant, however the addition makes the ar2 coefficient non significant.  Next we will try removing the ar2 coefficient
sig(fit101) #all three coefficients are significant, will try adding another ma term.
sig(fit102) #this yields a second insignificant ma coefficeint
AIC(fit200, fit101)
BIC(fit200, fit101)
summary(fit200) #RMSE 13.29
summary(fit101) #RMSE 12.83
fit200$coef
plot(as.vector(fitted(fit200)), fit200$residuals)
abline(h = 0, lty = 2)
qqnorm(fit200$residuals)
qqline(fit200$residuals)
Acf(fit200$residuals) #nothing significant here, independence is okay
Box.test(fit200$residuals, type = "Ljung-Box") #p-value is insignificant therefore we can retain the null hypothesis that the correlations among the residuals are zero, therefore the model is sufficient.
new.values <- forecast(fit200, 10, level = c(60, 80, 95))
new.values
plot(new.values)
