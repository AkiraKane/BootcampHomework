---
title: "Time Series Analysis"
author: "Joe Eckert"
date: "October 15, 2015"
output: html_document
---

###Question #1: ARIMA Modelling

```{r}

library(TSA)
library(forecast)
library(tseries)

sig = function(model){
  
  return(round((1 - pnorm(abs(model$coef)/sqrt(diag(model$var.coef)))) * 2, 3))
  
}

```


For each of the listed datasets, follow the procedure below to fit an ARIMA model.

####The hare dataset in the TSA library (load by calling data(hare)); this dataset describes the annual number of Canadian hare near the Hudson bay area.

```{r}

data(hare)

```

#####1.Plot the original time series

```{r}

plot(hare)

```

#####2.Determine the estimated number of differences necessary to make the time series stationary.  If non-stationary, plot the differenced time series.

```{r}

ndiffs(hare)

```

d = 0

#####3.Run the Augmented Dicky-Fuller test and interpret the results.  If necessary, return to part 2.

```{r}

adf.test(hare)

```

Close enough to yielding a significant p-value; move forward without differencing.

#####4.Plot the autocorrelation and partial autocorrelation functions to determine intial values of p & q.

```{r}

Acf(hare)
Pacf(hare)

```

Let's start with AR(2) and an MA(0); p = 2, q = 0 initially.

#####5.Fit an initial ARIMA model.

```{r}

fit200 = Arima(hare, c(2, 0, 0))
summary(fit200)
sig(fit200)

```

#####6.Follow the process of overfitting the initial ARIMA model you fit in part 5.  At each step, examine the Wald test statistics for the coefficient estimates.  (NB: If you seem to have already overfit your model, try taking away terms by reversing the overfitting process).

```{r}

fit300 = Arima(hare, order = c(3, 0, 0))
summary(fit300)
sig(fit300) #p = 3, coefficients are not significant, will use p = 2 and move onto q

fit201 = Arima(hare, order = c(2, 0, 1))
summary(fit201)
sig(fit201) #ma term is not significant, however it is low enough we will leave it an add another ar term

fit301 = Arima(hare, order = c(3, 0, 1))
summary(fit301)
sig(fit301) #third ar term is not significant, so we dont want to add the third ar term, try another ma term

fit202 = Arima(hare, order = c(2, 0, 2))
summary(fit202)
sig(fit202) #2nd ma term is not significant, revert back to fit201

```

#####7.Evaluate the family of models you created in part 6 and select the best (examine the AIC, BIC, and RMSE).

```{r}

AIC(fit200, fit300, fit201)
BIC(fit200, fit300, fit201)

summary(fit200) #RMSE 13.29
summary(fit300) #RMSE 12.83
summary(fit201) #RMSE 12.78

```

Everything is similar, we will choose the most parsimonious model (fit200).

#####8.Loosely interpret the coefficient estimates of the model you selected in part 7.

```{r}

fit200$coef

```

The intercept is 38, this means that the data averages around 38.  The ar1 coefficient of 1.3 means that the t-1 value impacts the current value by 1.3x, and the t-2 value impacts the current value by -.79.

#####9. Perform diagnostic tests for the best model by assessing:

######a. The constant variance assumption (graphically).

```{r}

plot(as.vector(fitted(fit200)), fit200$residuals)
abline(h = 0, lty = 2)

```

Maybe a violation of constant variance, not clear.

######b. The normality assumption (graphically).

```{r}

qqnorm(fit200$residuals)
qqline(fit200$residuals)

```

Might be an issue as well.

######c. The independence assumption (graphically & statistically).

```{r}

Acf(fit200$residuals) #nothing significant here, independence is okay
Box.test(fit200$residuals, type = "Ljung-Box") #p-value is insignificant therefore we can retain the null hypothesis that the correlations among the residuals are zero, therefore the model is sufficient.

```


#####10. Forecast the next few observations and display this graphically.

```{r}

new.values <- forecast(fit200, 10, level = c(60, 80, 95))
new.values

plot(new.values)

```

Be careful to note that it's predicting negative values, which is not possible.

#####11. How will the Canadian hare population change over time?

The model shows that the seasonality continues with an initial drop in population and then a spike in 1940.


####The WWWusage dataset; this dataset describes internet usage by logging the number of users connected to a server every minute.

```{r}

WWWusage

```

#####1.Plot the original time series

```{r}

plot(WWWusage)

```

#####2.Determine the estimated number of differences necessary to make the time series stationary.  If non-stationary, plot the differenced time series.

```{r}

ndiffs(WWWusage)
WWWusageDif <- diff(WWWusage, differences = 1)
ndiffs(WWWusageDif)
```

d = 1
Differencing once results in ndiffs = 0

#####3.Run the Augmented Dicky-Fuller test and interpret the results.  If necessary, return to part 2.

```{r}

adf.test(WWWusageDif)
WWWusageDif2 <- diff(WWWusageDif, differences = 1)
adf.test(WWWusageDif2)

```

The p-value is .35 and not significant therefore the differenced model is not stationary.  I will re do the difference again and test the adf.

The second differenced model results in a significant p value of .01, therefore we will use this model going forward.

#####4.Plot the autocorrelation and partial autocorrelation functions to determine intial values of p & q.

```{r}

Acf(WWWusageDif2)
Pacf(WWWusageDif2)

```

Let's start with AR(1) and an MA(1); p = 1, q = 1 initially.

#####5.Fit an initial ARIMA model.

```{r}

fit121 = Arima(WWWusageDif2, c(1, 2, 1))
summary(fit121)
sig(fit121)

#Here the AR term is not significant, I will move on to p = 2

```

#####6.Follow the process of overfitting the initial ARIMA model you fit in part 5.  At each step, examine the Wald test statistics for the coefficient estimates.  (NB: If you seem to have already overfit your model, try taking away terms by reversing the overfitting process).

```{r}

fit221 = Arima(WWWusageDif2, order = c(2, 2, 1))
summary(fit221)
sig(fit221) # here the coefficients are significant.  I will try increase the q value to 2 to see the impact

fit222 = Arima(WWWusageDif2, order = c(2, 2, 2))
summary(fit222)
sig(fit222) #by adding an additional ma term the significance of the first ar term is increased.  I will work with this model.

```

#####7.Evaluate the family of models you created in part 6 and select the best (examine the AIC, BIC, and RMSE).

```{r}

AIC(fit121, fit221, fit222)
BIC(fit121, fit221, fit222)

summary(fit121) #RMSE 4.56
summary(fit221) #RMSE 3.95
summary(fit222) #RMSE 3.20

```

fit222 has the lowest values for AIC, BIC and RMSE, we will proceed with this model.

#####8.Loosely interpret the coefficient estimates of the model you selected in part 7.

```{r}

fit222$coef

```

The ar1 coefficient of .27 means that the t-1 value impacts the current value by .27x, and the t-2 value impacts the current value by -.43.  The ma1 coefficient means that error terms from t-1 impact the current value by -1.20x and the t-2 error terms impact the current value by 1.00x.

#####9. Perform diagnostic tests for the best model by assessing:

######a. The constant variance assumption (graphically).

```{r}

plot(as.vector(fitted(fit222)), fit222$residuals)
abline(h = 0, lty = 2)

```

Maybe a violation of constant variance, not clear (seems that resuduals are centered around the middle.

######b. The normality assumption (graphically).

```{r}

qqnorm(fit222$residuals)
qqline(fit222$residuals)

```

Does not appear to be a violation of normality

######c. The independence assumption (graphically & statistically).

```{r}

Acf(fit222$residuals) #nothing significant here, independence is okay
Box.test(fit222$residuals, type = "Ljung-Box") #p-value is insignificant therefore we can retain the null hypothesis that the correlations among the residuals are zero, therefore the model is sufficient.

```


#####10. Forecast the next few observations and display this graphically.

```{r}

new.values <- forecast(fit222, 10, level = c(60, 80, 95))
new.values

plot(new.values)

```

Be careful to note that it's predicting negative values, which is not possible.

#####11. How will server traffic change over the next few minutes?

There appears to be a slight drop and tehn stables out after a  few observations.

####The nhtemp dataset; this dataset describes the average yearly temperatures (Â°F) in New Haven, Connecticut.

```{r}

nhtemp <- nhtemp

```

#####1.Plot the original time series

```{r}

plot(nhtemp)

```

#####2.Determine the estimated number of differences necessary to make the time series stationary.  If non-stationary, plot the differenced time series.

```{r}

ndiffs(nhtemp)
nhtempDif <- diff(nhtemp, differences = 1)
ndiffs(nhtempDif)

```

d = 1
differenced ndiff = 0

#####3.Run the Augmented Dicky-Fuller test and interpret the results.  If necessary, return to part 2.

```{r}

adf.test(nhtempDif)

```

The p value for the differenced model is significant, we can move forward with this model.

#####4.Plot the autocorrelation and partial autocorrelation functions to determine intial values of p & q.

```{r}

Acf(nhtempDif)
Pacf(nhtempDif)

```

Let's start with AR(1) and an MA(0); p = 1, q = 0 initially.

#####5.Fit an initial ARIMA model.

```{r}

fit110 = Arima(nhtempDif, c(1, 1, 0))
summary(fit110)
sig(fit110)

```

The ar1 term here is significant, therefore I will use this model going forward.

#####6.Follow the process of overfitting the initial ARIMA model you fit in part 5.  At each step, examine the Wald test statistics for the coefficient estimates.  (NB: If you seem to have already overfit your model, try taking away terms by reversing the overfitting process).

```{r}

fit111 = Arima(nhtempDif, c(1, 1, 1))
summary(fit111)
sig(fit111) #all coefficients significant

fit211 = Arima(nhtempDif, c(2, 1, 1))
summary(fit211)
sig(fit211) #ar2 not significant

fit212 = Arima(nhtempDif, c(2, 1, 2))
summary(fit212)
sig(fit212) #ar2 not significant

fit112 = Arima(nhtempDif, c(1, 1, 2))
summary(fit112)
sig(fit112) # all coefficients are significant

fit113 = Arima(nhtempDif, c(1, 1, 3))
summary(fit113)
sig(fit113) #all coeffcients are significant

fit114 = Arima(nhtempDif, c(1, 1, 4))
summary(fit114)
sig(fit114) #ma4 is not significant

```

The models with significant coefficients are fit111, fit112, fit113

#####7.Evaluate the family of models you created in part 6 and select the best (examine the AIC, BIC, and RMSE).

```{r}

AIC(fit110, fit111, fit112, fit113)
BIC(fit110, fit111, fit112, fit113)

summary(fit110) #RMSE 1.77
summary(fit111) #RMSE 1.22
summary(fit112) #RMSE 1.12
summary(fit113) #RMSE 1.07

```

Model fit113 has both the lowest AIC, BIC and RMSE, I will proceed with this model.

#####8.Loosely interpret the coefficient estimates of the model you selected in part 7.

```{r}

fit113$coef

```

The ar1 coefficient of -1.00 means that the t-1 value impacts the current value by -1.00x, the ma1 coefficient of -.75 means that error terms of t-1 impact the current value by -.75 times, ma2 (t-2) impacts by -.99x and ma3 (t-3) impacts by .74x.

#####9. Perform diagnostic tests for the best model by assessing:

######a. The constant variance assumption (graphically).

```{r}

plot(as.vector(fitted(fit113)), fit113$residuals)
abline(h = 0, lty = 2)

```

There appears to be no violation of constant variance.

######b. The normality assumption (graphically).

```{r}

qqnorm(fit113$residuals)
qqline(fit113$residuals)

```

Does not appear to be a violation of normality.

######c. The independence assumption (graphically & statistically).

```{r}

Acf(fit113$residuals) #nothing significant here, independence is okay
Box.test(fit113$residuals, type = "Ljung-Box") #p-value is insignificant therefore we can retain the null hypothesis that the correlations among the residuals are zero, therefore the model is sufficient.

```


#####10. Forecast the next few observations and display this graphically.

```{r}

new.values <- forecast(fit113, 10, level = c(60, 80, 95))
new.values

plot(new.values)

```

The next ten predicted observations appear to be highly volatile.

#####11. How will the weather change in New Haven Connecticut over the next few years?

Temperature changes are quite volatile going forward, which may be attributed to seasonality changes.

####The brobs.csv dataset; this dataset describes the monthly armed robberies in Boston.

Hint #1: Use the following commands to read the dataset correctly and create a time series object:
brobs = read.csv("07brobs.csv")
brobs = ts(brobs[119, 2], start = 1966, frequency = 12)
Hint #2: Comment on why you might want to log transform this data before proceeding with the ARIMA procedure.

```{r}

brobs = read.csv('07brobs.csv')
brobs = ts(brobs[-119, 2], start = 1966, frequency = 12)

```

#####1.Plot the original time series

```{r}

plot(brobs)
# The plot appears to show minor exponential growth of the data over time, therefore it might be a good idea to log transform the data.

brobslog <- log(brobs)
plot(brobslog)
# the plot of the logged set appears to be more linear in nature

```

#####2.Determine the estimated number of differences necessary to make the time series stationary.  If non-stationary, plot the differenced time series.

```{r}

ndiffs(brobslog)
brobslogDif <- diff(brobslog, differences = 1)
ndiffs(brobslogDif)
```

d = 1
differentiated model results in ndiffs = 0, will continue with the first differentiated model.

#####3.Run the Augmented Dicky-Fuller test and interpret the results.  If necessary, return to part 2.

```{r}

adf.test(brobslogDif)

```

Yielding a significant p-value; move forward without further differencing.

#####4.Plot the autocorrelation and partial autocorrelation functions to determine intial values of p & q.

```{r}

Acf(brobslogDif)
Pacf(brobslogDif)

```

Let's start with AR(2) and an MA(0); p = 2, q = 0 initially.

#####5.Fit an initial ARIMA model.

```{r}

fit200 = Arima(brobslogDif, c(2, 0, 0))
summary(fit200)
sig(fit200)

```

#####6.Follow the process of overfitting the initial ARIMA model you fit in part 5.  At each step, examine the Wald test statistics for the coefficient estimates.  (NB: If you seem to have already overfit your model, try taking away terms by reversing the overfitting process).

```{r}

fit300 = Arima(brobslogDif, order = c(3, 0, 0))
summary(fit300)
sig(fit300) #p = 3, coefficients are not significant, will use p = 2 and move onto q

fit201 = Arima(brobslogDif, order = c(2, 0, 1))
summary(fit201)
sig(fit201) #ma term is not significant, however the addition makes the ar2 coefficient non significant.  Next we will try removing the ar2 coefficient

fit101 = Arima(brobslogDif, order = c(1, 0, 1))
summary(fit101)
sig(fit101) #all three coefficients are significant, will try adding another ma term.

fit102 = Arima(brobslogDif, order = c(1, 0, 2))
summary(fit102)
sig(fit102) #this yields a second insignificant ma coefficeint
```

#####7.Evaluate the family of models you created in part 6 and select the best (examine the AIC, BIC, and RMSE).

```{r}

AIC(fit200, fit101)
BIC(fit200, fit101)

summary(fit200) #RMSE 0.20
summary(fit101) #RMSE 0.18

```

The AIC and BIC are lower for the fit200 model, there is only a slight increase in the RMSE value, so I will stick with the more parsimonious model fit200.

#####8.Loosely interpret the coefficient estimates of the model you selected in part 7.

```{r}

fit200$coef

```

The intercept is .02, this means that the data averages around .02 when x is 0.  The ar1 coefficient of -.28 means that the t-1 value impacts the current value by -.28x, and the t-2 value impacts the current value by -.22.

#####9. Perform diagnostic tests for the best model by assessing:

######a. The constant variance assumption (graphically).

```{r}

plot(as.vector(fitted(fit200)), fit200$residuals)
abline(h = 0, lty = 2)

```

Maybe a violation of constant variance, not clear.

######b. The normality assumption (graphically).

```{r}

qqnorm(fit200$residuals)
qqline(fit200$residuals)

```

Might be an issue as well.

######c. The independence assumption (graphically & statistically).

```{r}

Acf(fit200$residuals) #nothing significant here, independence is okay
Box.test(fit200$residuals, type = "Ljung-Box") #p-value is insignificant therefore we can retain the null hypothesis that the correlations among the residuals are zero, therefore the model is sufficient.

```


#####10. Forecast the next few observations and display this graphically.

```{r}

new.values <- forecast(fit200, 10, level = c(60, 80, 95))
new.values

plot(new.values)

```

#####11. How will the Boston crime scene change over the next few years?

Based on the predicted values, the growth rate (given that data is log transformed) of crime in Chicago would be flat over the next few months.