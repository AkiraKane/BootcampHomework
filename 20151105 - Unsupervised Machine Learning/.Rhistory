#only 4 terminal nodes with an accuracy of about 71.67%.
##########################
#####Regression Trees#####
##########################
#Inspecting the housing values in the suburbs of Boston.
library(MASS)
help(Boston)
#Creating a training set on 70% of the data.
set.seed(0)
train = sample(1:nrow(Boston), 7*nrow(Boston)/10)
#Training the tree to predict the median value of owner-occupied homes (in $1k).
tree.boston = tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
#Visually inspecting the regression tree.
plot(tree.boston)
text(tree.boston, pretty = 0)
#Performing cross-validation.
set.seed(0)
cv.boston = cv.tree(tree.boston)
par(mfrow = c(1, 2))
plot(cv.boston$size, cv.boston$dev, type = "b",
xlab = "Terminal Nodes", ylab = "RSS")
plot(cv.boston$k, cv.boston$dev, type  = "b",
xlab = "Alpha", ylab = "RSS")
#Pruning the tree to have 4 terminal nodes.
prune.boston = prune.tree(tree.boston, best = 4)
par(mfrow = c(1, 1))
plot(prune.boston)
text(prune.boston, pretty = 0)
#Calculating and assessing the MSE of the test data on the overall tree.
yhat = predict(tree.boston, newdata = Boston[-train, ])
yhat
boston.test = Boston[-train, "medv"]
boston.test
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
#Calculating and assessing the MSE of the test data on the pruned tree.
yhat = predict(prune.boston, newdata = Boston[-train, ])
yhat
plot(yhat, boston.test)
abline(0, 1)
mean((yhat - boston.test)^2)
##################################
#####Bagging & Random Forests#####
##################################
library(randomForest)
#Fitting an initial random forest to the training subset.
set.seed(0)
rf.boston = randomForest(medv ~ ., data = Boston, subset = train, importance = TRUE)
rf.boston
#The MSE and percent variance explained are based on out-of-bag estimates, a
#yielding unbiased error estimates. The model reports that mtry = 4, which is
#the number of variables randomly chosen at each split. Since we have 13 overall
#variables, we could try all 13 possible values of mtry. We will do so, record
#the results, and make a plot.
#Varying the number of variables used at each step of the random forest procedure.
set.seed(0)
oob.err = double(13)
test.err = double(13)
for (mtry in 1:13) {
fit = randomForest(medv ~ ., data = Boston, subset = train, mtry = mtry)
oob.err[mtry] = fit$mse[500]
pred = predict(fit, Boston[-train, ])
test.err[mtry] = with(Boston[-train, ], mean((medv - pred)^2))
cat("We're performing iteration", mtry, "\n")
}
#Visualizing the test error and the OOB error.
matplot(1:13, cbind(test.err, oob.err),
pch = 16, col = c("red", "blue"), type = "b",
xlab = "Variables at Each Split", ylab = "Mean Squared Error")
legend("topright", legend = c("Test", "OOB"), pch = 19, col = c("red", "blue"))
#Can visualize a variable importance plot.
importance(rf.boston)
varImpPlot(rf.boston)
##################
#####Boosting#####
##################
library(gbm)
#Fitting 10,000 trees with a depth of 4.
set.seed(0)
boost.boston = gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4)
#Inspecting the relative influence.
par(mfrow = c(1, 1))
summary(boost.boston)
#Partial dependence plots for specific variables; these plots illustrate the
#marginal effect of the selected variables on the response after integrating
#out the other variables.
par(mfrow = c(1, 2))
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
#As the number of higher prooportion of lower status people in the suburb, the
#lower the value of the housing prices. As the number of rooms increases, so
#does the expected house price.
#Letâ€™s make a prediction on the test set. With boosting, the number of trees is
#a tuning parameter; having too many can cause overfitting. In general, we should
#use cross validation to select the number of trees. Instead, we will compute the
#test error as a function of the number of trees and make a plot for illustrative
#purposes.
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = Boston[-train, ], n.trees = n.trees)
#Produces 100 different predictions for each of the 152 observations in our
#test set.
dim(predmat)
#Calculating the boosted errors.
par(mfrow = c(1, 1))
berr = with(Boston[-train, ], apply((predmat - medv)^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
#Include the best test error from the random forest.
abline(h = min(test.err), col = "red")
#Increasing the shrinkage parameter; a higher proportion of the errors are
#carried over.
set.seed(0)
boost.boston2 = gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.1)
predmat2 = predict(boost.boston2, newdata = Boston[-train, ], n.trees = n.trees)
berr2 = with(Boston[-train, ], apply((predmat2 - medv)^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
n.trees = seq(from = 100, to = 10000, by = 1)
predmat = predict(boost.boston, newdata = Boston[-train, ], n.trees = n.trees)
#Produces 100 different predictions for each of the 152 observations in our
#test set.
dim(predmat)
#Calculating the boosted errors.
par(mfrow = c(1, 1))
berr = with(Boston[-train, ], apply((predmat - medv)^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
#Include the best test error from the random forest.
abline(h = min(test.err), col = "red")
#Increasing the shrinkage parameter; a higher proportion of the errors are
#carried over.
set.seed(0)
boost.boston2 = gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.1)
predmat2 = predict(boost.boston2, newdata = Boston[-train, ], n.trees = n.trees)
berr2 = with(Boston[-train, ], apply((predmat2 - medv)^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = Boston[-train, ], n.trees = n.trees)
#Produces 100 different predictions for each of the 152 observations in our
#test set.
dim(predmat)
#Calculating the boosted errors.
par(mfrow = c(1, 1))
berr = with(Boston[-train, ], apply((predmat - medv)^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
#Include the best test error from the random forest.
abline(h = min(test.err), col = "red")
#Increasing the shrinkage parameter; a higher proportion of the errors are
#carried over.
set.seed(0)
boost.boston2 = gbm(medv ~ ., data = Boston[train, ],
distribution = "gaussian",
n.trees = 10000,
interaction.depth = 4,
shrinkage = 0.1)
predmat2 = predict(boost.boston2, newdata = Boston[-train, ], n.trees = n.trees)
berr2 = with(Boston[-train, ], apply((predmat2 - medv)^2, 2, mean))
plot(n.trees, berr2, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
setwd("~/Dropbox/NYCDSA/Homework/20151105 - Unsupervised Machine Learning")
install.pacakges(HSAUR)
install.pacakges('HSAUR')
install.packages('HSAUR')
library(HSAUR)
Data(heptahlon)
data(heptahlon)
heptathlon
head(heptathlon)
hept <- heptathlon
head(hept)
plot(hept)
hept[,'hurdles'] = max(hept[,'hurdles']) - hept[,'hurdles']
hept[,'run200m'] = max(hept[,'run200m']) - hept[,'run200m']
hept[,'run800m'] = max(hept[,'run800m']) - hept[,'run800m']
plot(hept)
names(hept)
hept.noscr = hept[,-8]
names(hept.noscr)
?fa.parallel
?fa.paralell
library(psych)
?fa.paralell
??fa.paralell
??fa.parallel
fa.parallel(hept.noscr, fa="pc", n.iter=100)
?principal()
hept.pca = principal(hept.noscr, nfactors=2, rotate="none")
hept.pca
names(hept.pca)
hept.pca$loadings
hept.pca$loadings
factor.plot(hept.pca, labels = colnames(hept.noscr))
plot(hept.pca)
plot(hept.pca$scores)
hept.pca$scores
identify(hept.pca$scores)
hept[,25]
hept[25,]
tail(hept)
min(hept$score)
hept[,score=min(hept$score)]
hept[,score==min(hept$score)]
hept[,hept$score==min(hept$score)]
View(hept)
hept[hept$score==min(hept$score),]
hept[25,]
hept[hept$score==min(hept$score),]
prost <- read.csv('08prostate.txt')
head(prost)
prost <- read.table('08prostate.txt', header=TRUE)
head(prost)
set.seed(0)
train <- sample(1:nrow(prost), 8*nrow(prost)/10)
test <- (-train)
len(train)/len(prost)
length(train)/nrow(prost)
library(glmnet)
?model.matrix
prost.train <- prost[train, -lpsa]
prost.train <- prost[train, -prost$lpsa]
prost.train <- prost[train, ]
prost.train <- prost[train, -9]
prost.train <- prost[train, ]
prost.test <- prost[test, -9]
prost.x <- model.matrix(lpsa ~ ., prost.train)
prost.y <- prost.train$lpsa
grid = 10^seq(5, -2, length = 100)
?seq
prost.ridge = glmnet(prost.x, prost.y, alpha = 0, lambda = grid)
dim(coef(prost.ridge)) #20 different coefficients, estimated 100 times --
summary(prost)
View(prost.x)
prost.x <- model.matrix(lpsa ~ ., prost.train)[,-1]
prost.y <- prost.train$lpsa
grid = 10^seq(5, -2, length = 100)
prost.ridge = glmnet(prost.x, prost.y, alpha = 0, lambda = grid)
dim(coef(prost.ridge)) #20 different coefficients, estimated 100 times --
#once each per lambda value.
coef(prost.ridge) #Inspecting the various coefficient estimates.
prost.ridge <- glmnet(prost.x, prost.y, alpha = 0, lambda = grid)
dim(coef(prost.ridge)) #20 different coefficients, estimated 100 times --
#once each per lambda value.
prost.ridge.coef <- coef(prost.ridge) #Inspecting the various coefficient estimates.
?data.frame()
prost.ridge.coef <- coef(prost.ridge)
plot(prost.ridge, xvar = "lambda", label = TRUE, main = "Ridge Regression")
prost.ridge.cv = cv.glmnet(prost.x, prost.y, alpha = 0, nfolds = 10)
set.seed(0)
prost.ridge.cv = cv.glmnet(prost.x, prost.y, alpha = 0, nfolds = 10)
prost.ridge.cv
plot(prost.ridge.cv, main = "Ridge Regression\n")
plot(prost.ridge.cv, main = "Ridge Regression\n")
bestlambda.ridge = prost.ridge.cv$lambda.min
bestlambda.ridge
log(bestlambda.ridge)
prost.ridge.bl <- predict(prost.ridge.cv, s = bestlambda.ridge, newx = model.matrix(lpsa ~ ., prost.test)[,-1])
prost.ridge.bl <- predict(prost.ridge.cv, s = bestlambda.ridge, newx = pred.test)
prost.ridge.bl <- predict(prost.ridge.cv, s = bestlambda.ridge, newx = prost.test)
prost.x.test <- model.matrix(lpsa ~ ., prost.test)
prost.x.test <- model.matrix(lpsa ~ ., prost.test)
prost.ridge.best = predict(prost.ridge, s = bestlambda.ridge, newx = prost.test)
prost.ridge.best = predict(prost.ridge, s = bestlambda.ridge, newx = prost.test)
?predict
prost.ridge.best = predict(prost.ridge, s = bestlambda.ridge, newx = prost.test)
class(prost.test)
class(as.matrix(prost.test))
prost.ridge.best = predict(prost.ridge, s = bestlambda.ridge, newx = as.matrix(prost.test))
mean((prost.ridge.best - prost[test, 9])^2)
plot(prost.ridge.cv, main = "Ridge Regression\n")
mean((prost.ridge.best - prost[test, 9])^2)
prost.ridge.all = glmnet(prost[,-9], prost[,9], alpha = 0)
prost.all.x = prost[,-9]
prost.all.y = prost[,9]
prost.all.x = model.matrix(lpsa ~ ., prost)[,-1]
prost.all.y = prost[,9]
prost.ridge.all = glmnet(prost.all.x, prost[,9], alpha = 0)
predict(ridge.all, type = "coefficients", s = bestlambda.ridge)
predict(prost.ridge.all, type = "coefficients", s = bestlambda.ridge)
prost.ridge.all.pred = predict(prost.ridge.all, s = bestlambda.ridge, newx = prost.all.x)
mean((prost.ridge.all.pred - prost[,9])^2)
?"glmnet"
prost.x <- model.matrix(lpsa ~ ., prost.train)[,-1]
prost.y <- prost.train$lpsa
grid <- 10^seq(5, -2, length = 100)
prost.lasso <- glmnet(prost.x, prost.y, alpha = 1, lambda = grid)
dim(coef(prost.lasso))
prost.lasso.coef <- coef(prost.lasso)
plot(prost.lasso, xvar = "lambda", label = TRUE, main = "Lasso Regression")
set.seed(0)
prost.lasso.cv = cv.glmnet(prost.x, prost.y, alpha = 0, nfolds = 10)
plot(prost.lasso.cv, main = "Ridge Regression\n")
plot(prost.lasso.cv, main = "Lasso Regression\n")
set.seed(0)
prost.lasso.cv = cv.glmnet(prost.x, prost.y, alpha = 1, nfolds = 10)
plot(prost.lasso.cv, main = "Lasso Regression\n")
bestlambda.lasso = prost.lasso.cv$lambda.min
bestlambda.lasso
log(bestlambda.lasso)
prost.lasso.best = predict(prost.lasso, s = bestlambda.lasso, newx = as.matrix(prost.test))
mean((prost.lasso.best - prost[test, 9])^2)
prost.all.x = model.matrix(lpsa ~ ., prost)[,-1]
prost.all.y = prost[,9]
prost.lasso.all = glmnet(prost.all.x, prost[,9], alpha = 1)
predict(prost.lasso.all, type = "coefficients", s = bestlambda.lasso)
prost.lasso.all.pred = predict(prost.lasso.all, s = bestlambda.lasso, newx = prost.all.x)
mean((prost.lasso.all.pred - prost[,9])^2)
protein = read.table("08protein.txt", sep = 't', header = TRUE)
protein = read.table("08protein.txt", header = TRUE)
protein = read.table("08protein.txt", sep = '\t', header = TRUE)
View(protein)
protein.scaled = as.data.frame(scale(protein[, -1]))
rownames(protein.scaled) = protein$Country
View(protein.scaled)
wssplot = function(data, nc = 15, seed = 0) {
wss = (nrow(data) - 1) * sum(apply(data, 2, var))
for (i in 2:nc) {
set.seed(seed)
wss[i] = sum(kmeans(data, centers = i, iter.max = 100, nstart = 100)$withinss)
}
plot(1:nc, wss, type = "b",
xlab = "Number of Clusters",
ylab = "Within-Cluster Variance",
main = "Scree Plot for the K-Means Procedure")
}
wssplot(protein.scaled)
set.seed(0)
km.p1 = kmeans(protein.scaled, centers = 3)
km.p2 = kmeans(protein.scaled, centers = 3)
km.p3 = kmeans(protein.scaled, centers = 3)
km.p4 = kmeans(protein.scaled, centers = 3)
km.p5 = kmeans(protein.scaled, centers = 3)
set.seed(0)
km.p = kmeans(protein.scaled, centers = 3, nstart = 100)
par(mfrow = c(2, 3))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.protein1$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 1: ", round(km.protein1$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.protein2$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 2: ", round(km.protein2$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.protein3$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 3: ", round(km.protein3$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.protein4$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 4: ", round(km.protein4$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.protein5$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 5: ", round(km.protein5$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.protein$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("K-Means with 100 Attempts: ", round(km.protein$tot.withinss, 4)))
par(mfrow = c(2, 3))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p1$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 1: ", round(km.p1$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p2$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 2: ", round(km.p2$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p3$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 3: ", round(km.p3$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p4$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 4: ", round(km.p4$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.5$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 5: ", round(km.p5$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("K-Means with 100 Attempts: ", round(km.p$tot.withinss, 4)))
par(mfrow = c(2, 3))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p1$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 1: \n", round(km.p1$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p2$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 2: \n", round(km.p2$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p3$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 3: \n", round(km.p3$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p4$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 4: \n", round(km.p4$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.5$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 5: \n", round(km.p5$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("K-Means with 100 Attempts: \n", round(km.p$tot.withinss, 4)))
par(mfrow = c(2, 3))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p1$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 1: \n", round(km.p1$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p2$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 2: \n", round(km.p2$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p3$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 3: \n", round(km.p3$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p4$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 4: \n", round(km.p4$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p5$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("Single K-Means Attempt 5: \n", round(km.p5$tot.withinss, 4)))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p$cluster,
xlab = "Cereals", ylab = "Red Meat",
main = paste("K-Means with 100 Attempts: \n", round(km.p$tot.withinss, 4)))
par(mfrow = c(1,1))
par(mfrow = c(1,1))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p$cluster,
xlab = "Cereals", ylab = "Red Meat", type = "n",
main = paste("Best K-Means Attempt out of 100\n WCV: ", round(km.p$tot.withinss, 4)))
points(km.protein$centers[, 6], km.protein$centers[, 1], pch = 16, col = c(1:3))
abline(h=0)
abline(v=0)
text(protein.scaled$Cereals, protein.scaled$RedMeat, labels=row.names(protein.scaled), col = km.protein$cluster)
```
par(mfrow = c(1,1))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p$cluster,
xlab = "Cereals", ylab = "Red Meat", type = "n",
main = paste("Best K-Means Attempt out of 100\n WCV: ", round(km.p$tot.withinss, 4)))
points(km.p$centers[, 6], km.p$centers[, 1], pch = 16, col = c(1:3))
abline(h=0)
abline(v=0)
text(protein.scaled$Cereals, protein.scaled$RedMeat, labels=row.names(protein.scaled), col = km.protein$cluster)
par(mfrow = c(1,1))
plot(protein.scaled$Cereals, protein.scaled$RedMeat, col = km.p$cluster,
xlab = "Cereals", ylab = "Red Meat", type = "n",
main = paste("Best K-Means Attempt out of 100\n WCV: ", round(km.p$tot.withinss, 4)))
points(km.p$centers[, 6], km.p$centers[, 1], pch = 16, col = c(1:3))
abline(h=0)
abline(v=0)
text(protein.scaled$Cereals, protein.scaled$RedMeat, labels=row.names(protein.scaled), col = km.p$cluster)
dist = dist(protein.scaled)
dist = dist(protein.scaled)
```
####2. Fit hierarchical clustering solutions using single, complete, and average linkage.
```{r}
fit.single = hclust(dist, method = "single")
fit.complete = hclust(dist, method = "complete")
fit.average = hclust(dist, method = "average")
```
####3. Visualize the dendrograms created in part 2.
a. Give an argument as to why single linkage might not be good to use.
b. Give an argument as to why complete linkage might be good to use.
```{r}
par(mfrow = c(1, 3))
plot(fit.single, hang = -1, main = "Dendrogram of Single Linkage")
plot(fit.complete, hang = -1, main = "Dendrogram of Complete Linkage")
plot(fit.average, hang = -1, main = "Dendrogram of Average Linkage")
par(mfrow = c(1,1))
clusters.c2 = cutree(fit.complete, k = 2)
clusters.c2
table(clusters.c2)
aggregate(protein.scaled, by = list(cluster = clusters.c2), median)
plot(fit.complete, hang = -1, main = "Dendrogram of Average Linkage\n2 Clusters")
rect.hclust(fit.complete, k = 2)
clusters.c5 = cutree(fit.complete, k = 5)
clusters.c5
table(clusters.c5)
aggregate(protein.scaled, by = list(cluster = clusters.c5), median)
plot(fit.complete, hang = -1, main = "Dendrogram of Average Linkage\n2 Clusters")
rect.hclust(fit.complete, k = 5)
clusters.c2 = cutree(fit.complete, k = 2)
clusters.c2
table(clusters.c2)
aggregate(protein.scaled, by = list(cluster = clusters.c2), median)
plot(fit.complete, hang = -1, main = "Dendrogram of Average Linkage\n2 Clusters")
rect.hclust(fit.complete, k = 2)
```{r}
clusters.c5 = cutree(fit.complete, k = 5)
clusters.c5
table(clusters.c5)
aggregate(protein.scaled, by = list(cluster = clusters.c5), median)
plot(fit.complete, hang = -1, main = "Dendrogram of Average Linkage\n2 Clusters")
rect.hclust(fit.complete, k = 5)
```
clusters.c5 = cutree(fit.complete, k = 5)
clusters.c5
table(clusters.c5)
aggregate(protein.scaled, by = list(cluster = clusters.c5), median)
plot(fit.complete, hang = -1, main = "Dendrogram of Average Linkage\n2 Clusters")
rect.hclust(fit.complete, k = 5)
