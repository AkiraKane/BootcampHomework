nycMapCoord <- c(-74.30,40.45, -73.65, 40.95)
nycMap <- get_map(location = nycMapCoord, source="google", maptype="roadmap")
noiseGrp <- group_by(select(noiseRnd, -Created.Date), areaCoord)
# Plot heat map by year
mapYear <- ggmap(nycMap)
mapYear <- mapYear + stat_density2d(aes(x=lonRnd, y=latRnd, fill="red", alpha=..level.., size=0), data=noiseGrp, geom = 'polygon')
mapYear <- mapYear + facet_wrap(~ year) + ggtitle("Frequency of Noise Complaints by Location")
mapYear <- mapYear + theme(axis.ticks = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank(), legend.position="none")
mapYear <- mapYear + xlab("") + ylab("")
mapYear
# Population by Zip Code
popzipData <- read.csv('popzip.csv', skip =1)
popzip <- select(popzipData, Geography, Geographic.area.1, Population)
popzip <- mutate(popzip, zipcode = substr(Geographic.area.1, 6, 11))
popzip$zipcode <- as.integer(popzip$zipcode)
popzip <- filter(popzip, !is.na(zipcode))
popzip <- tbl_df(popzip)
names(popzip)[4] <- "zip"
data(zipcode)
zipcode <- tbl_df(zipcode)
zipcode$zip <- as.numeric(zipcode$zip)
# Summarise noise data by zip and year
noiseZip <- group_by(select(noiseRnd, -Created.Date), year, Incident.Zip)
noiseZip <- summarise(noiseZip, count = n())
names(noiseZip)[2] <- "zip"
noiseZip <- tbl_df(noiseZip)
noiseZip <- left_join(noiseZip, popzip)
noiseZip <- left_join(noiseZip, zipcode)
noiseZip <- select(noiseZip, -Geography, -Geographic.area.1, -city, -state)
noiseZip <- filter(noiseZip, !is.na(Population))
noiseZip$Population <- as.numeric(noiseZip$Population)
noiseZip$callpercap <- (noiseZip$count / noiseZip$Population)
# Plot frequency of complaints per capita
mapYearPop <- ggmap(nycMap)
mapYearPop <- mapYearPop + stat_density2d(aes(x=longitude, y=latitude, fill= "callpercap", alpha=..level.., size=0), data=noiseZip, geom = 'polygon')
mapYearPop <- mapYearPop + facet_wrap(~ year)
mapYearPop <- mapYearPop + ggtitle("Frequency of Complaints Per Capita (2010 Census)")
mapYearPop <- mapYearPop + theme(axis.ticks = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank(), legend.position="none")
mapYearPop <- mapYearPop + xlab("") + ylab("")
mapYearPop
# Prepare data for daily analysis
dataforcal <- select(noise, Created.Date, Unique.Key)
dataforcal <- mutate(dataforcal, Created.Date = as.character(Created.Date))
dataforcal <- mutate(dataforcal, Created.Date = substr(Created.Date, 0, 10))
dataforcal <- group_by(dataforcal, Created.Date)
dataforcal <- summarise(dataforcal, count = n())
names(dataforcal)[1] <- "date"
dataforcal$date <- as.POSIXct(dataforcal$date)
# Noise complaint volume 2010 - 2015
compVol <- qplot(date, count, data = dataforcal, geom = "smooth", xlab = "Date", ylab = "Daily Complaints", main = "Daily Complaints 2010 - September 2015") + theme_bw()
compVol
# Plot 2014 data on a calendar heatmap
calPlot <- calendarPlot(dataforcal, "count", year = 2014, main = "Noise Complaints by Day 2014")
calPlot
# Complaints by month
noiseMonth <- group_by(select(noiseRnd, -Created.Date), month)
noiseMonth <- filter(noiseMonth, year == 2014)
noiseMonth <- summarise(noiseMonth, count = n())
monthlyComp <- qplot(noiseMonth$month, noiseMonth$count, noiseMonth, geom = "bar", stat = "identity")
monthlyComp <- monthlyComp + ggtitle("Frequency of Complaints by Month 2014")
monthlyComp <- monthlyComp + xlab("Month") + ylab("Number of Complaints")
monthlyComp <- monthlyComp + theme_bw()
monthlyComp <- monthlyComp + scale_x_discrete(limits = noiseMonth$month, labels = c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"))
monthlyComp <- monthlyComp + theme(axis.text.x=element_text(angle=-45))
monthlyComp
# Complaints by weekday
noiseWkdy <- group_by(select(noiseRnd, -Created.Date), wkDay)
noiseWkdy <- filter(noiseMonth, year == 2014)
noiseWkdy <- summarise(noiseWkdy, count = n())
wkdyComp <- qplot(noiseWkdy$wkDay, noiseWkdy$count, noiseWkdy, geom = "bar", stat = "identity")
wkdyComp <- wkdyComp + ggtitle("Frequency of Complaints by Weekday 2014")
wkdyComp <- wkdyComp + xlab("Weekday") + ylab("Number of Complaints")
wkdyComp <- wkdyComp + theme_bw() +
wkdyComp <- wkdyComp + scale_x_discrete(limits = noiseWkdy$wkDay, labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday"))
wkdyComp <- wkdyComp + theme(axis.text.x=element_text(angle=-45))
wkdyComp
## COMPARISON OF COMPLAINT TYPES
type <- group_by(select(noiseRnd, -Created.Date), Complaint.Type, Descriptor)
type <- summarise(type, count = n())
type$typeClean <- c(rep(NA, length(type$Complaint.Type)))
type$typeClean[grepl('music', type$Descriptor, ignore.case = TRUE)] <- "Loud Music"
type$typeClean[grepl('talking', type$Descriptor, ignore.case = TRUE)] <- "Loud Talking"
type$typeClean[grepl('construction|hammer', type$Descriptor, ignore.case = TRUE)] <- "Construction"
type$typeClean[grepl('dog|animal', type$Descriptor, ignore.case = TRUE)] <- "Animal"
type$typeClean[grepl('banging', type$Descriptor, ignore.case = TRUE)] <- "Banging/Pounding"
type$typeClean[grepl('horn|vehicle|truck|engine', type$Descriptor, ignore.case = TRUE)] <- "Vehicle"
type$typeClean[grepl('alarm', type$Descriptor, ignore.case = TRUE)] <- "Alarm"
type$typeClean[grepl('air con', type$Descriptor, ignore.case = TRUE)] <- "HVAC Equipment"
type$typeClean[type$Complaint.Type == "Noise - Helicopter"] <- "Aircraft"
type$typeClean[is.na(type$typeClean)] <- "Other"
type <- ungroup(type)
noiseType <- left_join(select(noiseRnd, -Created.Date), type, by = c("Complaint.Type", "Descriptor"))
noiseTypeP <- group_by(noiseType, month, typeClean)
noiseTypeP <- filter(noiseTypeP, year == 2014)
noiseTypeP <- summarise(noiseTypeP, count = n())
noiseTypePlot <- qplot(noiseTypeP$month, noiseTypeP$count, noiseTypeP, geom = "bar", fill = noiseTypeP$typeClean, stat = "identity")
noiseTypePlot <- noiseTypePlot + ggtitle("Frequency of Complaints by Month & Type 2014")
noiseTypePlot <- noiseTypePlot + xlab("Month") + ylab("Number of Complaints")
noiseTypePlot <- noiseTypePlot + theme_bw()
noiseTypePlot <- noiseTypePlot + scale_x_discrete(limits = noiseMonth$month, labels = c("January", "February", "March", "April", "May", "June", "July", "August", "September", "October", "November", "December"))
noiseTypePlot <- noiseTypePlot + theme(axis.text.x=element_text(angle=-45))
noiseTypePlot$labels$fill <- "Complaint Type"
noiseTypePlot
boroughCalls
mapYear
?calendarPlot
calPlot
calPlot <- calendarPlot(dataforcal, "count", year = 2014, main = "Noise Complaints by Day 2014", cols = "blue")
calPlot <- calendarPlot(dataforcal, "count", year = 2014, main = "Noise Complaints by Day 2014", cols = "3-class Blues")
calPlot
calPlot <- calendarPlot(dataforcal, "count", year = 2014, main = "Noise Complaints by Day 2014", cols = "increment")
calPlot
##########################################################
##########################################################
#####[02] Missingness, Imputation, & KNN Lecture Code#####
##########################################################
##########################################################
##################################
#####Visualizing Missing Data#####
##################################
library(VIM) #For the visualization and imputation of missing values.
help(sleep) #Inspecting the mammal sleep dataset.
sleep
summary(sleep) #Summary information for the sleep dataset.
sapply(sleep, sd) #Standard deviations for the sleep dataset; any issues?
aggr(sleep) #A graphical interpretation of the missing values and their
#combinations within the dataset.
library(mice) #Load the multivariate imputation by chained equations library.
md.pattern(sleep) #Can also view this information from a data perspective.
###############################
#####Mean Value Imputation#####
###############################
#Creating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
#Mean value imputation method 1.
missing.data$x2[is.na(missing.data$x2)] = mean(missing.data$x2, na.rm=TRUE)
missing.data
#Mean value imputation method 2.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
missing.data = transform(missing.data, x2 = ifelse(is.na(x2),
mean(x2, na.rm=TRUE),
x2))
missing.data
#Mean value imputation method 3.
library(Hmisc) #Load the Harrell miscellaneous library.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10))) #Recreating dataset.
imputed.x2 = impute(missing.data$x2, mean) #Specifically calling the x2 variable.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
##################################
#####Simple Random Imputation#####
##################################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
mean(missing.data$x2, na.rm = TRUE) #Mean of x2 prior to imputation.
sd(missing.data$x2, na.rm = TRUE) #Standard deviation of x2 prior to imputation.
cor(missing.data, use = "complete.obs") #Correlation prior to imputation.
set.seed(0)
imputed.x2 = impute(missing.data$x2, "random") #Simple random imputation using the
#impute() function from the Hmisc package.
imputed.x2
summary(imputed.x2) #Summary information for the imputed variable.
is.imputed(imputed.x2) #Boolean vector indicating imputed values.
missing.data$x2 = imputed.x2 #Replacing the old vector.
mean(missing.data$x2) #Mean of x2 after imputation.
sd(missing.data$x2) #Standard deviation of x2 after imputation.
cor(missing.data, use = "complete.obs") #Correlation afterto imputation.
plot(missing.data) #What are some potential problems with mean value imputation?
#############################
#####K-Nearest Neighbors#####
#############################
#Recreating a dataset that has missing values.
missing.data = data.frame(x1 = 1:20, x2 = c(1:10, rep(NA, 10)))
missing.data
imputed.1nn = kNN(missing.data, k = 1) #Imputing using 1NN.
imputed.5nn = kNN(missing.data, k = 5) #Imputing using 5NN.
imputed.9nn = kNN(missing.data, k = 9) #Imputing using 9NN.
imputed.1nn #Inspecting the imputed values of each of the methods;
imputed.5nn #what is going on here? Given our dataset, should we
imputed.9nn #expect these results?
#K-Nearest Neighbors regression on the sleep dataset.
sqrt(nrow(sleep)) #Determining K for the sleep dataset.
sleep.imputed8NN = kNN(sleep, k = 8) #Using 8 nearest neighbors.
sleep.imputed8NN
summary(sleep) #Summary information for the original sleep dataset.
summary(sleep.imputed8NN[, 1:10]) #Summary information for the imputed sleep dataset.
#K-Nearest Neighbors classification on the iris dataset.
help(iris) #Inspecting the iris measurement dataset.
iris
iris.example = iris[, c(1, 2, 5)] #For illustration purposes, pulling only the
#sepal measurements and the flower species.
#Throwing some small amount of noise on top of the data for illustration
#purposes; some observations are on top of each other.
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
missing.vector = c(41:50, 91:100, 141:150) #Inducing missing values on the Species
iris.example$Species[missing.vector] = NA  #vector for each category.
iris.example
col.vec[missing.vector] = "purple" #Creating a new color vector to
#mark the missing values.
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
plot.tile.list(tile.list(info),
fillcol = col.vec[-missing.vector],
main = "Iris Voronoi Tessellation\nDecision Boundaries")
#Adding the observations that are missing species information.
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = 16, col = "white")
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = "?", cex = .66)
#Conducting a 1NN classification imputation.
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
#Conducting a 12NN classification imputation based on the square root of n.
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed12NN$Species)
##################################################
#####Using Minkowski Distance Measures in KNN#####
##################################################
library(kknn) #Load the weighted knn library.
#Separating the complete and missing observations for use in the kknn() function.
complete = iris.example[-missing.vector, ]
missing = iris.example[missing.vector, -3]
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
iris.manhattan = kknn(Species ~ ., complete, missing, k = 12, distance = 1)
summary(iris.manhattan)
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
#Conducting a 12NN classification imputation based on the square root of n.
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed12NN$Species)
set.seed(0)
iris.example$Sepal.Length = jitter(iris.example$Sepal.Length, factor = .5)
iris.example$Sepal.Width = jitter(iris.example$Sepal.Width, factor= .5)
col.vec = c(rep("red", 50), #Creating a color vector for plotting purposes.
rep("green", 50),
rep("blue", 50))
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica"),
pch = 16, col = c("red", "green", "blue"), cex = .75)
missing.vector = c(41:50, 91:100, 141:150) #Inducing missing values on the Species
iris.example$Species[missing.vector] = NA  #vector for each category.
iris.example
col.vec[missing.vector] = "purple" #Creating a new color vector to
#mark the missing values.
plot(iris.example$Sepal.Length, iris.example$Sepal.Width,
col = col.vec, pch = 16,
main = "Sepal Measurements of Iris Data")
legend("topleft", c("Setosa", "Versicolor", "Virginica", "NA"),
pch = 16, col = c("red", "green", "blue", "purple"), cex = .75)
#Inspecting the Voronoi tesselation for the complete observations in the iris
#dataset.
library(deldir) #Load the Delaunay triangulation and Dirichelet tesselation library.
info = deldir(iris.example$Sepal.Length[-missing.vector],
iris.example$Sepal.Width[-missing.vector])
plot.tile.list(tile.list(info),
fillcol = col.vec[-missing.vector],
main = "Iris Voronoi Tessellation\nDecision Boundaries")
#Adding the observations that are missing species information.
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = 16, col = "white")
points(iris.example$Sepal.Length[missing.vector],
iris.example$Sepal.Width[missing.vector],
pch = "?", cex = .66)
#Conducting a 1NN classification imputation.
iris.imputed1NN = kNN(iris.example, k = 1)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed1NN$Species)
#Conducting a 12NN classification imputation based on the square root of n.
sqrt(nrow(iris.example))
iris.imputed12NN = kNN(iris.example, k = 12)
#Assessing the results by comparing to the truth known by the original dataset.
table(iris$Species, iris.imputed12NN$Species)
library(kknn) #Load the weighted knn library.
#Separating the complete and missing observations for use in the kknn() function.
complete = iris.example[-missing.vector, ]
missing = iris.example[missing.vector, -3]
#Distance corresponds to the Minkowski power.
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
iris.manhattan = kknn(Species ~ ., complete, missing, k = 12, distance = 1)
summary(iris.manhattan)
iris.euclidean = kknn(Species ~ ., complete, missing, k = 12, distance = 2)
summary(iris.euclidean)
iris.manhattan = kknn(Species ~ ., complete, missing, k = 12, distance = 1)
summary(iris.manhattan)
install.packages("PASWR")
setwd("~/Dropbox/NYCDSA/Data/Week 03 Data")
#####Manual example with the cars dataset#####
##############################################
help(cars)
cars #Investigating the cars dataset.
#Basic numerical EDA for cars dataset.
summary(cars) #Five number summaries.
sapply(cars, sd) #Standard deviations.
cor(cars) #Correlations.
#Basic graphical EDA for cars dataset.
hist(cars$speed, xlab = "Speed in MPH", main = "Histogram of Speed")
hist(cars$dist, xlab = "Distance in Feet", main = "Histogram of Distance")
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
#Manual calculation of simple linear regression coefficients.
beta1 = sum((cars$speed - mean(cars$speed)) * (cars$dist - mean(cars$dist))) /
sum((cars$speed - mean(cars$speed))^2)
beta0 = mean(cars$dist) - beta1*mean(cars$speed)
#Adding the least squares regression line to the plot.
abline(beta0, beta1, lty = 2)
#Calculating the residual values.
residuals = cars$dist - (beta0 + beta1*cars$speed)
#Note the sum of the residuals is 0.
sum(residuals)
#Visualizing the residuals.
segments(cars$speed, cars$dist,
cars$speed, (beta0 + beta1*cars$speed),
col = "red")
text(cars$speed - .5, cars$dist, round(residuals, 2), cex = 0.5)
#################################################
#####Automatic example with the cars dataset#####
#################################################
model = lm(dist ~ speed, data = cars) #Use the linear model function lm() to
summary(model) #All the summary information for the model in question. Reports:
t.statistic = 9.464
f.statistic = 89.57
t.statistic^2
confint(model) #Creating 95% confidence intervals for the model coefficients.
####################################################
#####Checking assumptions with the cars dataset#####
####################################################
#Linearity
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
abline(model, lty = 2)
#Constant Variance & Independent Errors
plot(model$fitted, model$residuals,
xlab = "Fitted Values", ylab = "Residual Values",
main = "Residual Plot for Cars Dataset")
abline(h = 0, lty = 2)
#Normality
qqnorm(model$residuals)
qqline(model$residuals)
#Using the built-in plot() function to visualize the residual plots.
plot(model) #Note the addition of the loess smoother and scale-location plot
library(car) #Companion to applied regression.
influencePlot(model)
influencePlot(model)
model$fitted.values #Returns the fitted values.
newdata = data.frame(speed = c(15, 20, 25)) #Creating a new data frame to pass
View(newdata)
predict(model, newdata, interval = "confidence") #Construct confidence intervals
#for the average value of an
#outcome at a specific point.
predict(model, newdata, interval = "prediction") #Construct prediction invervals
newdata = data.frame(speed = 4:25)
conf.band = predict(model, newdata, interval = "confidence")
pred.band = predict(model, newdata, interval = "prediction")
View(conf.band)
#Visualizing the confidence and prediction bands.
plot(cars, xlab = "Speed in MPH", ylab = "Distance in Feet",
main = "Scatterplot of Cars Dataset")
abline(model, lty = 2) #Plotting the regression line.
lines(newdata$speed, conf.band[, 2], col = "blue") #Plotting the lower confidence band.
lines(newdata$speed, conf.band[, 3], col = "blue") #Plotting the upper confidence band.
lines(newdata$speed, pred.band[, 2], col = "red") #Plotting the lower prediction band.
lines(newdata$speed, pred.band[, 3], col = "red") #Plotting the upper prediction band.
legend("topleft", c("Regression Line", "Conf. Band", "Pred. Band"),
lty = c(2, 1, 1), col = c("black", "blue", "red"))
bc = boxCox(model) #Automatically plots a 95% confidence interval for the lambda
#value that maximizes the likelihhood of transforming to
#normality.
lambda = bc$x[which(bc$y == max(bc$y))] #Extracting the best lambda value.
dist.bc = (cars$dist^lambda - 1)/lambda #Applying the Box-Cox transformation.
model.bc = lm(dist.bc ~ cars$speed) #Creating a new regression based on the
#transformed variable.
summary(model.bc) #Assessing the output of the new model.
plot(model.bc) #Assessing the assumptions of the new model.
boxCox(model.bc) #What happens if we want to apply the Box-Cox transformation
bc = boxCox(model) #Automatically plots a 95% confidence interval for the lambda
boxCox(model.bc) #What happens if we want to apply the Box-Cox transformation
setwd("~/Dropbox/NYCDSA/Homework/20151007 - Simple Linear Regression")
library(MASS)
cats <- cats
plot(cats$Hwt, cats$Bwt)
catslm <- lm(Bwt ~ Hwt, data = cats)
catslm
summary(catslm)
plot(cats$Bwt, cats$Hwt)
catslm <- lm(Hwt ~ Bwt, data = cats)
summary(catslm)
?cats
abline(catslm, lty = 2)
abline(catslm, lty = 2, col = "red")
plot(cats$Bwt, cats$Hwt, xlab = "Body weight", ylab = "Heart weight", title = "Body weight vs Heart weight")
plot(cats$Bwt, cats$Hwt, xlab = "Body weight", ylab = "Heart weight", main = "Body weight vs Heart weight")
abline(catslm, lty = 2, col = "red")
abline(catslm$residuals, lty = 3, col = "blue")
abline(catslm$residuals, lty = 2, col = "blue")
abline(catslm$residuals)
resid <- residuals(catslm)
pred <- predict(catslm)
segments(cats$Bwt, cats$Hwt, cats$Bwt, pred, col = "red")
?segments
summary(catslm)
predict(catslm, interval = "confidence")
?predict
newdata <- data.frame(Bwt = 0:4)
summary(cats)
0.0:3.9
conf.band <- predict(catslm, newdata, interval = "confidence")
conf.band
lines(newdata$Bwt, conf.band[,2], col = "blue")
lines(newdata$Bwt, conf.band[,3], col = "blue")
legend("topleft", c("Regression Line", "Conf. Band"), lty = c(2, 1), col = c("red", "blue"))
summary(catslm)
confint(catslm)
plot(catslm$fitted, catslm$residuals, xlab = "Fitted Values", ylab = "Residual Values", main = "Residual Plot for Cats")
abline(h = 0, lty = 2, col = "red")
plot(cats$Bwt, cats$Hwt, xlab = "Body weight", ylab = "Heart weight", main = "Body weight vs Heart weight")
xlab = "Fitted Values", ylab = "Residual Values",
main = "Residual Plot for Cars Dataset")
plot(model$fitted, model$residuals,
xlab = "Fitted Values", ylab = "Residual Values",
main = "Residual Plot for Cars Dataset")
abline(h = 0, lty = 2)
plot(catslm$fitted, catslm$residuals, xlab = "Fitted Values", ylab = "Residual Values", main = "Residual Plot for Cats")
abline(h = 0, lty = 2, col = "red")
qqnorm(catslm$residuals)
plot(cats$Bwt, cats$Hwt, xlab = "Body weight", ylab = "Heart weight", main = "Body weight vs Heart weight")
abline(catslm, lty = 2, col = "red")
pred.band <- predict(catslm, newdata, interval = "predict")
plot(cats$Bwt, cats$Hwt, xlab = "Body weight", ylab = "Heart weight", main = "Body weight vs Heart weight")
abline(catslm, lty = 2, col = "green")
newdata <- data.frame(Bwt = 0:4)
conf.band <- predict(catslm, newdata, interval = "confidence")
pred.band <- predict(catslm, newdata, interval = "predict")
lines(newdata$Bwt, conf.band[,2], col = "blue")
lines(newdata$Bwt, conf.band[,3], col = "blue")
lines(newdata$Bwt, pred.band[,2], col = "red")
lines(newdata$Bwt, pred.band[,3], col = "red")
legend("topleft", c("Regression Line", "Conf. Band", "Pred Band"), lty = c(2, 1), col = c("green", "blue", "red"))
?predict
newdata <- c(2.8, 5, 10)
predict(catslm, newdata, interval = "confidence")
predict(catslm, newdata, interval = "predict")
newdata <- data.frame(Bwt = c(2.8, 5, 10))
predict(catslm, newdata, interval = "confidence")
predict(catslm, newdata, interval = "predict")
catsbc <- boxCox(catslm)
lambda = catsbc$x[which(catsbc$y == max(catsbc$y))]
catsbcT = (cats$Bwt^lambda - 1)/lambda
catslmBC <- lm(catsbcT ~ cats$Hwt)
cbind(catsbcT, cats$Hwt)
catsbcTran <- cbind(catsbcT, cats$Hwt)
plot(catsbcTran[,1], catsbcTran[,2])
plot(catsbcTran[,1], catsbcTran[,2], xlab = "Transformed Body weight", ylab = "Heart weight")
plot(catsbcTran[,1], catsbcTran[,2], xlab = "Transformed Body weight", ylab = "Heart weight")
abline(model.bc, lty = 2, col = "green")
model.bc = lm(dist.bc ~ cars$speed) #Creating a new regression based on the
#transformed variable.
plot(catsbcTran[,1], catsbcTran[,2], xlab = "Transformed Body weight", ylab = "Heart weight")
abline(model.bc, lty = 2, col = "green")
plot(catsbcTran[,1], catsbcTran[,2], xlab = "Transformed Body weight", ylab = "Heart weight")
abline(catslmBC, lty = 2, col = "green")
catslmBC <- lm(catsbcT ~ cats$Hwt)
# Question 2.5
catsbcTran <- cbind(catsbcT, cats$Hwt)
plot(catsbcTran[,1], catsbcTran[,2], xlab = "Transformed Body weight", ylab = "Heart weight")
abline(catslmBC, lty = 2, col = "green")
summary(catslmBC)
plot(catsbcTran[,1], catsbcTran[,2], xlab = "Transformed Body weight", ylab = "Heart weight")
abline(catslmBC, lty = 2, col = "green")
abline(catslmBC, lty = 2, col = "green")
